<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="UTF-8">
    <title>Natural Zoom Gesture on Frugal Devices</title>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" type="text/css" href="stylesheet/normalize.css" media="screen">
    <link href='https://fonts.googleapis.com/css?family=Open+Sans:400,700' rel='stylesheet' type='text/css'>
    <link rel="stylesheet" type="text/css" href="stylesheet/stylesheet.css" media="screen">
    <link rel="stylesheet" type="text/css" href="stylesheet/github-light.css" media="screen">
  </head>
  <body>
    <section class="page-header">
      <h1 class="project-name">Zoom Gesture</h1>
      <h2 class="project-tagline">Natural Zoom Gesture Interaction on Frugal Devices for AR Application</h2>
      <a href="https://youtu.be/at8uBJiehQ8" class="btn">Demo Video</a>
     
    </section>

    <section class="main-content">
      <h3>
<a id="welcome-to-ZoomGesture" class="anchor" href="#welcome-to-telepresence-roi" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Welcome to TeleAssist</h3>

<p>In air-gesture based Natural interaction has important role in human-computer interface. It Provides significant part in non verbal communication, hand gestures are playing vital role in our dailylife. Gesture Recognition has a wide area of application including human machine interaction, sign language, immersive game technology etc. However, many applications have been proposed to incorporate with additional wearable sensors and some specific training for each user may be required. These limitations make it inconvenience for using in the real world. This paper presents novel one-handed gesture based interaction for zoom in/out manipulation of FOV, which is fast and easy to implement without using any additional. The manipulation of zoom command with natural gesture is to pinch or to stretch fingers, like on touch-devices. The goal of this research is to extending the interaction space around mobile devices for augmented reality on Cardboard, i.e. applications where users look at the live image of the devices video camera.The technique uses only the RGB camera now commonplace on off-the-shelf mo- bile devices. Our algorithm robustly recognizes Zoom in/out in-air gestures, supporting user variation, and varying lighting conditions. We demonstrate that our algorithm runs in real-time on unmodified mobile devices, including resource-constrained Smartphones.</p>

<h3>
<a id="the-idea" class="anchor" href="#the-idea" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>The Idea</h3>

<p><img src="https://github.com/arc4224/AirGesturesROI/blob/master/img/Setup.png?raw=true?" alt="Proposed Method "></p>

<p>We  present an  approach  for  marker-less  and  real-time touch-less gestures to mark ROI on wearables in FPV by proposing a novel two  stage  sequential  gesture  recognition  method as shown in the demo video.   First,  we  detect  a dynamic  gesture which involves detecting the presence of a stable hand , then raising the index finger with the rest of the fist closed(termed as point gesture) to trigger ROI Selection. This is followed by another dynamic gesture involving moving point gesture around the object of interest.   Our  approach  is  particularly  suitable  as  most  of  the smartphones available in the market are not equipped with built-in depth sensor posing additional challenges. The main blocks of the algorithm are: (i)point gesture detection, (ii) ROI selection, (iii) ROI tracking, and (iv) subsequent updating of bounding box around the ROI. Skin pixel detection followed by largest contour segmentation gives the hand region in the user's FOV. The fingertip is computed as the farthest point from the centroid of hand. After finger  tip  detection, a closed contour is drawn following the locus of the detected fingertip,  in the sequence of frames. After  the  ROI  is  selected,  the  resultant  bounding  box using an approach based on Shi-Tomasi feature points and optical flow vectors. The unreliable trajectories of the feature points are eliminated using the Forward-Backward Error method. 
We conducted experiments on  industrial which  demonstrates  that  the problem identification with our method of ROI highlighting gets faster by 60%, in comparison to sole audio instructions. </p>

<h3>
<a id="the-idea" class="anchor" href="#the-idea" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Demo Video</h3>
  <iframe width="560" height="315" src="https://www.youtube.com/embed/at8uBJiehQ8" frameborder="0" allowfullscreen></iframe>
      <footer class="site-footer">
        <span class="site-footer-owner"><a href="https://github.com/arc4224/AirGestures-ROI">Telepresence-ROI</a> is maintained by <a href="https://github.com/arc4224">arc4224</a>.</span>

      </footer>

    </section>

  
  </body>
</html>
